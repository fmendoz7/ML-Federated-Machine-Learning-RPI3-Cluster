{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "#Install PySyft in Google Colab\n",
        "\n",
        "get_ipython().system('pip install tf-encrypted==0.5.6')\n",
        "get_ipython().system('pip install msgpack==0.6.1')\n",
        "\n",
        "get_ipython().system(' URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;')\n",
        "\n",
        "get_ipython().system('cd PySyft; python setup.py install  > /dev/null')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "get_ipython().system('pip install --upgrade --force-reinstall lz4')\n",
        "get_ipython().system('pip install --upgrade --force-reinstall websocket')\n",
        "get_ipython().system('pip install --upgrade --force-reinstall websockets')\n",
        "get_ipython().system('pip install --upgrade --force-reinstall zstd')\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import torch\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import string\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import math\n",
        "import syft as sy\n",
        "import pandas as pd\n",
        "import random\n",
        "from syft.frameworks.torch.federated import utils\n",
        "\n",
        "from syft.workers import WebsocketClientWorker\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "get_ipython().system('wget https://download.pytorch.org/tutorial/data.zip  ')\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "get_ipython().system('unzip data.zip')\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "path = '/content/data/names/*.txt'\n",
        "\n",
        "all_letters = string.ascii_letters + \".,;'\"\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "#Load files in the path\n",
        "def findFiles(path):\n",
        "  return glob.glob(path)\n",
        "\n",
        "#Read a file and then split to lines\n",
        "def readLines(filename):\n",
        "  lines =open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "#Convert  string to ASCII format\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in all_letters\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "for filename in findFiles(path):\n",
        "  #print(filename)\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readLines(filename)\n",
        "  category_lines[category] = lines\n",
        "  \n",
        "n_categories = len(all_categories)\n",
        "\n",
        "#for names in glob.glob(path):\n",
        "  #print(names)\n",
        "  \n",
        " \n",
        "print(\"Number of categories: \" + \"\\n\" + str(n_categories))\n",
        "print(\"\\n\" + \"All categories: \")\n",
        "print(*all_categories, sep = \"\\n\")\n",
        "\n",
        "\n",
        "# \n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "print(*category_lines['Polish'][:6], sep = \"\\n\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "class LanguageDataset(Dataset):\n",
        "  # Constructor\n",
        "  def __init__(self, text, labels, transform=None):\n",
        "    self.data = text\n",
        "    self.targets = labels # categories\n",
        "    #self.to_torchtensor()\n",
        "    self.transform = transform\n",
        "    \n",
        "  def to_torchtensor(self):\n",
        "    self.data = torch.from_numpy(self.text, requires_grad=True)\n",
        "    self.labels = torch.from_numpy(self.targets, requires_grad=True)\n",
        "  \n",
        "  # Returns length of dataset/batches\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  # Returns data and target[torch tensor ]\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data[idx]\n",
        "    target = self.targets[idx]\n",
        "    \n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "      \n",
        "    return sample, target\n",
        "    \n",
        "  \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Arguments for the program\n",
        "class Arguments():\n",
        "  def __init__(self):\n",
        "    self.batch_size = 1\n",
        "    self.learning_rate = 0.005\n",
        "    self.epochs = 10000\n",
        "    self.federate_after_n_batches =15000\n",
        "    self.seed = 1\n",
        "    self.print_every = 200\n",
        "    self.plot_every = 100\n",
        "    self.use_cuda = False\n",
        "    \n",
        "args = Arguments()\n",
        "    \n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "get_ipython().run_cell_magic('latex', '', '\\n\\\\begin{split}\\nnames\\\\_list = [d_1,...d_n]  \\\\\\\\\\n\\ncategory\\\\_list = [c_1,...c_n]\\n\\\\end{split}')\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "names_list = []\n",
        "category_list = []\n",
        "\n",
        "for nation, names in category_lines.items():\n",
        "  for name in names:\n",
        "    names_list.append(name)\n",
        "    category_list.append(nation)\n",
        "    \n",
        "print(*names_list[:5], sep = \"\\n\")\n",
        "print(*category_list[:5], sep = \"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Data points loaded: \" + str(len(names_list)))\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "# An integer to every category\n",
        "categories_numerical = pd.factorize(category_list)[0]\n",
        "\n",
        "# Categories with tensor\n",
        "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
        "\n",
        "categories_numpy = np.array(category_tensor)\n",
        "\n",
        "print(names_list[100:120])\n",
        "print(categories_numpy[100:120])\n",
        "\n",
        "\n",
        "# We will turn every character in each input string into a vector, with a 1 marking that particular character present. <br>\n",
        "# A word will just be a vector of character vectors and our RNN will process every character vector in the word.<br>\n",
        "# This technique is called word embedding.\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# This returns the index of a letter given\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "    \n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor    \n",
        "    \n",
        "    \n",
        "# Turn a list of strings into a list of tensors\n",
        "def list_strings_to_list_tensors(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensor(line)\n",
        "        lineNumpy = lineTensor.numpy()\n",
        "        lines_tensors.append(lineNumpy)\n",
        "        \n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors(names_list)\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "# Testing the functions work\n",
        "print(names_list[0])\n",
        "print(lines_tensors[0])\n",
        "print(lines_tensors[0].shape)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Identify the longest word in the dataset as all tensors need to have the same\n",
        "# shape \n",
        "\n",
        "max_line_size = max(len(x) for x in lines_tensors)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "def lineToTensorFillEmpty(line, max_line_size):\n",
        "    tensor = torch.zeros(max_line_size, 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "        \n",
        "    # If there is no character, a vector with (0,0,.... ,0) will be placed\n",
        "    return tensor\n",
        "\n",
        "# Turn a list of strings into a list of tensors using previous function\n",
        "def list_strings_to_list_tensors_fill_empty(names_list):\n",
        "    lines_tensors = []\n",
        "    for index, line in enumerate(names_list):\n",
        "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
        "        lines_tensors.append(lineTensor)\n",
        "    return(lines_tensors)\n",
        "\n",
        "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "# Tensor shape check\n",
        "print(lines_tensors[0].shape)\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "# Create numpy array with all word embeddings\n",
        "array_lines_tensors = np.stack(lines_tensors)\n",
        "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
        "\n",
        "# Check array dimension\n",
        "print(array_lines_proper_dimension.shape)\n",
        "\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "def find_start_index_per_category(category_list):\n",
        "    categories_start_index = {}\n",
        "    \n",
        "    #Initialize every category with an empty list\n",
        "    for category in all_categories:\n",
        "        categories_start_index[category] = []\n",
        "    \n",
        "    #Insert the start index of each category into the dictionary categories_start_index\n",
        "    #Example: \"Italian\" --> 203\n",
        "    #         \"Spanish\" --> 19776\n",
        "    last_category = None\n",
        "    i = 0\n",
        "    for name in names_list:\n",
        "        cur_category = category_list[i]\n",
        "        if(cur_category != last_category):\n",
        "            categories_start_index[cur_category] = i\n",
        "            last_category = cur_category\n",
        "        \n",
        "        i = i + 1\n",
        "        \n",
        "    return(categories_start_index)\n",
        "\n",
        "categories_start_index = find_start_index_per_category(category_list)\n",
        "\n",
        "print(categories_start_index)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def randomChoice(l):\n",
        "    rand_value = random.randint(0, len(l) - 1)\n",
        "    return l[rand_value], rand_value\n",
        "\n",
        "\n",
        "def randomTrainingIndex():\n",
        "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
        "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
        "    line, rand_line_index = randomChoice(category_lines[category])\n",
        "    category_start_index = categories_start_index[category]\n",
        "    absolute_index = category_start_index + rand_line_index\n",
        "    return(absolute_index)\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "#Two hidden layers, based on simple linear layers\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "#Let's instantiate the neural network already:\n",
        "n_hidden = 128\n",
        "#Instantiate RNN\n",
        "\n",
        "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
        "model = RNN(n_letters, n_hidden, n_categories).to(device)\n",
        "#The final softmax layer will produce a probability for each one of our 18 categories\n",
        "print(model)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Specify remote workers's location\n",
        "\n",
        "hook = sy.TorchHook(torch)  # Hook PyTorch\n",
        "\n",
        "# Uncomment this with the ip of each raspberry pi worker if you're using the\n",
        "# raspberry pi and comment the block of code beneath this\n",
        "\n",
        "# kwargs_websocket_alice = {\"host\": \"ip_alice\", \"hook\": hook}\n",
        "# alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
        "# kwargs_websocket_bob = {\"host\": \"ip_bob\", \"hook\": hook}\n",
        "# bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
        "\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")  \n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")  \n",
        "\n",
        "workers_virtual = [alice, bob]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# array_lines_proper_dimension = our data points(X)\n",
        "# categories_numpy = our labels (Y)\n",
        "langDataset = LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
        "\n",
        "#assign the data points and the corresponding categories to workers.\n",
        "federated_train_loader = sy.FederatedDataLoader(\n",
        "    langDataset.federate(workers_virtual),\n",
        "    batch_size=args.batch_size)\n",
        "\n",
        "\n",
        "# # Model Training\n",
        "# Now the data is processed, we'll start to train our RNN!\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Gives the category that corresponds to maximum predicted class probability\n",
        "def categoryFromOutput(output):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    category_i = top_i[0].item()\n",
        "    return all_categories[category_i], category_i\n",
        "\n",
        "# Gives the amount of time passed since \"since\"\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "# Federated averaging\n",
        "def fed_avg_every_n_iters(model_pointers, iter, federate_after_n_batches):\n",
        "        models_local = {}\n",
        "        \n",
        "        if(iter % args.federate_after_n_batches == 0):\n",
        "            for worker_name, model_pointer in model_pointers.items():\n",
        "                # Assign model to the worker\n",
        "                models_local[worker_name] = model_pointer.copy().get()\n",
        "            model_avg = utils.federated_avg(models_local)\n",
        "           \n",
        "            for worker in workers_virtual:\n",
        "                model_copied_avg = model_avg.copy()\n",
        "                model_ptr = model_copied_avg.send(worker) \n",
        "                model_pointers[worker.id] = model_ptr\n",
        "                \n",
        "        return(model_pointers)     \n",
        "\n",
        "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
        "  \n",
        "    # Get the right initialized model\n",
        "    model_ptr = model_pointers[line_single.location.id]   \n",
        "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
        "    line_reshaped, category_single = line_reshaped.to(device), category_single.to(device)\n",
        "    \n",
        "    # Initialize hidden layer\n",
        "    hidden_init = model_ptr.initHidden() \n",
        "    \n",
        "    # And now zero the gradient\n",
        "    model_ptr.zero_grad()\n",
        "    hidden_ptr = hidden_init.send(line_single.location)\n",
        "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
        "    \n",
        "    # Forward passes\n",
        "    for i in range(amount_lines_non_zero): \n",
        "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
        "    criterion = nn.NLLLoss()   \n",
        "    loss = criterion(output, category_single) \n",
        "    loss.backward()\n",
        "    \n",
        "    model_got = model_ptr.get() \n",
        "    \n",
        "    # Update model's weights \n",
        "    for param in model_got.parameters():\n",
        "        param.data.add_(-args.learning_rate, param.grad.data)\n",
        "        \n",
        "        \n",
        "    # Send the model\n",
        "    model_sent = model_got.send(line_single.location.id)\n",
        "    model_pointers[line_single.location.id] = model_sent\n",
        "    \n",
        "    return(model_pointers, loss, output)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
        "    current_loss = 0\n",
        "    all_losses = []    \n",
        "    \n",
        "    model_pointers = {}\n",
        "    \n",
        "    # Send the initialized model to every single worker just before training\n",
        "    for worker in workers_virtual:\n",
        "        model_copied = model.copy()\n",
        "        model_ptr = model_copied.send(worker) \n",
        "        model_pointers[worker.id] = model_ptr\n",
        "\n",
        "    # Extract a random element from the list and perform training on it\n",
        "    for iter in range(1, n_iters + 1):        \n",
        "        random_index = randomTrainingIndex()\n",
        "        line_single, category_single = list_federated_train_loader[random_index]\n",
        "        line_name = names_list[random_index]\n",
        "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
        "        \n",
        "        # Update theloss\n",
        "        loss_got = loss.get().item() \n",
        "        current_loss += loss_got\n",
        "        \n",
        "        if iter % plot_every == 0:\n",
        "            all_losses.append(current_loss / plot_every)\n",
        "            current_loss = 0\n",
        "             \n",
        "        # Print information on training\n",
        "        # The name, guessed category, correct/incorrect and actual category\n",
        "        if(iter % print_every == 0):\n",
        "            output_got = output.get()\n",
        "            guess, guess_i = categoryFromOutput(output_got)\n",
        "            category = all_categories[category_single.copy().get().item()]\n",
        "            correct = '\u00e2\u0153\u201c' if guess == category else '\u00e2\u0153\u2014 (%s)' % category\n",
        "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, \n",
        "                                                    timeSince(start), \n",
        "                                                    loss_got, \n",
        "                                                    line_name, guess, correct))\n",
        "            \n",
        "            \n",
        "    return(all_losses, model_pointers)\n",
        "\n",
        "\n",
        "# ## Start the training\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "# Turn the data points and categories into a list\n",
        "list_federated_train_loader = list(federated_train_loader)\n",
        "\n",
        "# Start the training\n",
        "start = time.time()\n",
        "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, \n",
        "                                       args.plot_every, \n",
        "                                       args.federate_after_n_batches, \n",
        "                                       list_federated_train_loader)\n",
        "\n",
        "\n",
        "# In[25]:\n",
        "\n",
        "\n",
        "# Plot the loss we got during the training procedure\n",
        "plt.figure()\n",
        "plt.title(\"Loss over time training\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Epochs (100s)')\n",
        "plt.plot(all_losses)\n",
        "\n",
        "\n",
        "# ## Predict!\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def predict(model, input_line, all_categories, worker, n_predictions=3):\n",
        "    \"\"\" \n",
        "    Uses :attr:`model` to predict top :attr:`n_predictions` categories \n",
        "    from :attr:`all_categories` for :attr:`input_line` using :attr:`worker`\n",
        "  \n",
        "    Parameters: \n",
        "        model (Module): model to be used for the prediction\n",
        "        input_line (str): input to the model\n",
        "        all_categories (list): list of all categories for the prediction\n",
        "        worker(BaseWorker): worker where the prediction will be performed\n",
        "        n_predictions(int): number of top predictions to return\n",
        "  \n",
        "    Returns: \n",
        "        list of tuples (value, category) sorted from max value to min\n",
        "  \n",
        "    \"\"\"\n",
        "    \n",
        "    # copy the model to the worker only if is not already there\n",
        "    if model.location.id != worker.id:\n",
        "        model = model.copy().get()\n",
        "        model_remote = model.send(worker)\n",
        "    else:\n",
        "        model_remote = model\n",
        "    \n",
        "    # convert the input_line to a tensor and send it to the worker\n",
        "    line_tensor = lineToTensor(input_line)\n",
        "    line_remote = line_tensor.copy().send(worker)\n",
        "\n",
        "    # init the hidden layer\n",
        "    hidden = model_remote.initHidden()\n",
        "    hidden_remote = hidden.copy().send(worker)\n",
        "        \n",
        "    # get a result from the model\n",
        "    with torch.no_grad():\n",
        "        for i in range(line_remote.shape[0]):\n",
        "            output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
        "    \n",
        "    # get top N categories\n",
        "    topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
        "\n",
        "    # construct list of (value, category) tuples\n",
        "    predictions = []\n",
        "    for i in range(n_predictions):\n",
        "        value = topv[0][i].item()\n",
        "        category_index = topi[0][i].item()\n",
        "        #print('(%.2f) %s' % (value, all_categories[category_index]))\n",
        "        predictions.append([value, all_categories[category_index]])\n",
        "        \n",
        "    return predictions\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "print(predict(model_pointers[\"alice\"], \"Qing\", all_categories,  alice) )\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}