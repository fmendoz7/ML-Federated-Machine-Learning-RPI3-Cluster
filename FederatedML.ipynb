{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from easydict import EasyDict\n",
        "\n",
        "import ray\n",
        "from ray.tune.registry import register_env\n",
        "from ray import tune\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "def make_multiagent(args):\n",
        "    class MultiEnv(MultiAgentEnv):\n",
        "        def __init__(self):\n",
        "            self.agents = [gym.make(args.env) for _ in range(args.num_agents)]\n",
        "            self.dones = set()\n",
        "            self.observation_space = self.agents[0].observation_space\n",
        "            self.action_space = self.agents[0].action_space\n",
        "\n",
        "        def reset(self):\n",
        "            self.dones = set()\n",
        "            return {i: a.reset() for i, a in enumerate(self.agents)}\n",
        "\n",
        "        def step(self, action_dict):\n",
        "            obs, rew, done, info = {}, {}, {}, {}\n",
        "            for i, action in action_dict.items():\n",
        "                obs[i], rew[i], done[i], info[i] = self.agents[i].step(action)\n",
        "                if done[i]:\n",
        "                    self.dones.add(i)\n",
        "            done[\"__all__\"] = len(self.dones) == len(self.agents)\n",
        "            return obs, rew, done, info\n",
        "\n",
        "    return MultiEnv\n",
        "\n",
        "def make_fed_env(args):   \n",
        "    FedEnv = make_multiagent(args)\n",
        "    env_name = \"multienv_FedRL\"\n",
        "    register_env(env_name, lambda _: FedEnv())\n",
        "    return env_name\n",
        "\n",
        "def gen_policy_graphs(args):\n",
        "    single_env = gym.make(args.env)\n",
        "    obs_space = single_env.observation_space\n",
        "    act_space = single_env.action_space\n",
        "    policy_graphs = {'agent_{i}': (None, obs_space, act_space, {}) \n",
        "         for i in range(args.num_agents)}\n",
        "    return policy_graphs\n",
        "\n",
        "def policy_mapping_fn(agent_id):\n",
        "    return 'agent_{agent_id}'\n",
        "def change_weights(weights, i):\n",
        "    \"\"\"\n",
        "    Helper function for FedQ-Learning\n",
        "    \"\"\"\n",
        "    dct = {}\n",
        "    for key, val in weights.items():\n",
        "        # new_key = key\n",
        "        still_here = key[:6]\n",
        "        there_after = key[7:]\n",
        "        # new_key[6] = i\n",
        "        new_key = still_here + str(i) + there_after\n",
        "        dct[new_key] = val\n",
        "    # print(dct.keys())\n",
        "    return dct\n",
        "\n",
        "def synchronize(agent, weights, num_agents):\n",
        "    \"\"\"\n",
        "    Helper function to synchronize weights of the multiagent\n",
        "    \"\"\"\n",
        "    weights_to_set = {'agent_{i}': weights \n",
        "         for i in range(num_agents)}\n",
        "    # weights_to_set = {f'agent_{i}': change_weights(weights, i) \n",
        "    #    for i in range(num_agents)}\n",
        "    # print(weights_to_set)\n",
        "    agent.set_weights(weights_to_set)\n",
        "\n",
        "def uniform_initialize(agent, num_agents):\n",
        "    \"\"\"\n",
        "    Helper function for uniform initialization\n",
        "    \"\"\"\n",
        "    new_weights = agent.get_weights([\"agent_0\"]).get(\"agent_0\")\n",
        "    # print(new_weights.keys())\n",
        "    synchronize(agent, new_weights, num_agents)\n",
        "\n",
        "def compute_softmax_weighted_avg(weights, alphas, num_agents, temperature=1):\n",
        "    \"\"\"\n",
        "    Helper function to compute weighted avg of weights weighted by alphas\n",
        "    Weights and alphas must have same keys. Uses softmax.\n",
        "    params:\n",
        "        weights - dictionary\n",
        "        alphas - dictionary\n",
        "    returns:\n",
        "        new_weights - array\n",
        "    \"\"\"\n",
        "    def softmax(x, beta=temperature, length=num_agents):\n",
        "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "        e_x = np.exp(beta * (x - np.max(x)))\n",
        "        return (e_x / e_x.sum()).reshape(length, 1)\n",
        "    \n",
        "    alpha_vals = np.array(list(alphas.values()))\n",
        "    soft = softmax(alpha_vals)\n",
        "    weight_vals = np.array(list(weights.values()))\n",
        "    new_weights = sum(np.multiply(weight_vals, soft))\n",
        "    return new_weights\n",
        "\n",
        "def reward_weighted_update(agent, result, num_agents):\n",
        "    \"\"\"\n",
        "    Helper function to synchronize weights of multiagent via\n",
        "    reward-weighted avg of weights\n",
        "    \"\"\"\n",
        "    return softmax_reward_weighted_update(agent, result, num_agents, temperature=0)\n",
        "\n",
        "def softmax_reward_weighted_update(agent, result, num_agents, temperature=1):\n",
        "    \"\"\"\n",
        "    Helper function to synchronize weights of multiagent via\n",
        "    softmax reward-weighted avg of weights with specific temperature\n",
        "    \"\"\"\n",
        "    all_weights = agent.get_weights()\n",
        "    policy_reward_mean = result['policy_reward_mean']\n",
        "    episode_reward_mean = result['episode_reward_mean']\n",
        "    if policy_reward_mean:\n",
        "        new_weights = compute_softmax_weighted_avg(all_weights, policy_reward_mean, num_agents, temperature=temperature)\n",
        "        synchronize(agent, new_weights, num_agents)\n",
        "\n",
        "def fed_train(args):\n",
        "    temp_schedule = args.temp_schedule\n",
        "    temperature = temp_schedule[0]\n",
        "    hotter_temp = temp_schedule[1]\n",
        "    temp_shift = temp_schedule[2]\n",
        "    fed_schedule = args.fed_schedule\n",
        "    num_iters = fed_schedule[0]\n",
        "    increased_iters = fed_schedule[1]\n",
        "    fed_shift = fed_schedule[2]\n",
        "    \n",
        "    num_agents = args.num_agents\n",
        "    def fed_learn(info):\n",
        "#       get stuff out of info\n",
        "        result = info[\"result\"]\n",
        "        agent = info[\"trainer\"]\n",
        "        optimizer = agent.optimizer\n",
        "        if result['timesteps_total'] > fed_shift:\n",
        "            num_iters = increased_iters\n",
        "        if result['timesteps_total'] > temp_shift:\n",
        "            temperature = hotter_temp\n",
        "        # correct result reporting\n",
        "        result['episode_reward_mean'] = result['episode_reward_mean']/num_agents\n",
        "        result['episode_reward_max'] = result['episode_reward_max']/num_agents\n",
        "        result['episode_reward_min'] = result['episode_reward_min']/num_agents\n",
        "        result['federated'] = \"No federation\"\n",
        "        if result['training_iteration'] == 1:\n",
        "            uniform_initialize(agent, num_agents)\n",
        "        elif result['training_iteration'] % num_iters == 0:\n",
        "            result['federated'] = \"Federation with {temperature}\"\n",
        "            # update weights\n",
        "            softmax_reward_weighted_update(agent, result, num_agents, temperature)\n",
        "            # clear buffer, don't want smoothing here\n",
        "            optimizer.episode_history = []\n",
        "    return fed_learn\n",
        "\n",
        "def fedrl(args):\n",
        "    ray.init(ignore_reinit_error=True)\n",
        "    policy_graphs = gen_policy_graphs(args)\n",
        "    multienv_name = make_fed_env(args)\n",
        "    callback = fed_train(args)\n",
        "    tune.run(\n",
        "        args.algo,\n",
        "        name=\"{args.env}-{args.algo}-{args.num_agents}\",\n",
        "        stop={\"episode_reward_mean\": 9800},\n",
        "        config={\n",
        "                \"multiagent\": {\n",
        "                    \"policy_graphs\": policy_graphs,\n",
        "                    \"policy_mapping_fn\": tune.function(lambda agent_id: 'agent_{agent_id}'),\n",
        "                },\n",
        "                \"env\": multienv_name,\n",
        "                \"gamma\": 0.99,\n",
        "                \"lambda\": 0.95,\n",
        "                \"kl_coeff\": 1.0,\n",
        "                \"num_sgd_iter\": 32,\n",
        "                \"lr\": .0003 * args.num_agents,\n",
        "                \"vf_loss_coeff\": 0.5,\n",
        "                \"clip_param\": 0.2,\n",
        "                \"sgd_minibatch_size\": 4096,\n",
        "                \"train_batch_size\": 65536,\n",
        "                \"grad_clip\": 0.5,\n",
        "                \"batch_mode\": \"truncate_episodes\",\n",
        "                \"observation_filter\": \"MeanStdFilter\",\n",
        "                # \"lr\": tune.grid_search(args.lrs),\n",
        "#                 \"simple_optimizer\": True,\n",
        "                \"callbacks\":{\n",
        "                    \"on_train_result\": tune.function(callback),\n",
        "                },\n",
        "                \"num_workers\": args.num_workers,\n",
        "                \"num_gpus\": 1,\n",
        "            },\n",
        "        checkpoint_at_end=True\n",
        "    )\n",
        "\n",
        "\n",
        "args = EasyDict({\n",
        "    'num_agents': 5,\n",
        "    'num_workers': 7,\n",
        "    'fed_schedule': [1, 5, 2e7],\n",
        "    # 'temperatures': [0, 8, 0.5, 4, 2, 1, 16],\n",
        "    'temp_schedule': [0.5, 2, 2e7],\n",
        "    # 'timesteps': 1e7,\n",
        "    # 'lr': 5e-4,\n",
        "    # 'lrs': [5e-5, 5e-4, 5e-3],\n",
        "    # 'episodes': 150,\n",
        "#     'num_iters': 100,\n",
        "    'env': 'HalfCheetah-v2',\n",
        "    'name': 'fed_experiment',\n",
        "    'algo': 'PPO',\n",
        "})\n",
        "# train\n",
        "fedrl(args)\n",
        "# eval"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}